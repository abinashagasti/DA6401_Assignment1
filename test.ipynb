{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "9c4c45d1-ed26-48ca-a866-eca3316fa2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.multiprocessing as mp\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "3c81e930-45f3-47bf-bd15-30b117b0b58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, num_filters=[32, 64, 128, 256, 512], kernel_size=3, num_dense=256, activation=F.relu):\n",
    "        super(CNN, self).__init__()\n",
    "        self.activation = activation  # Set activation function\n",
    "\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, num_filters[0], kernel_size)\n",
    "        self.conv2 = nn.Conv2d(num_filters[0], num_filters[1], kernel_size)\n",
    "        self.conv3 = nn.Conv2d(num_filters[1], num_filters[2], kernel_size)\n",
    "        self.conv4 = nn.Conv2d(num_filters[2], num_filters[3], kernel_size)\n",
    "        self.conv5 = nn.Conv2d(num_filters[3], num_filters[4], kernel_size)\n",
    "\n",
    "        self.fc1 = nn.Linear(512*5*5 , 256)\n",
    "        # self.fc2 = nn.Linear(2048, 256)\n",
    "        self.fc3 = nn.Linear(256, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.activation(self.conv1(x)))\n",
    "        x = self.pool(self.activation(self.conv2(x)))\n",
    "        x = self.pool(self.activation(self.conv3(x)))\n",
    "        x = self.pool(self.activation(self.conv4(x)))\n",
    "        x = self.pool(self.activation(self.conv5(x)))\n",
    "\n",
    "        x = self.activation(self.fc1(torch.flatten(x, 1)))\n",
    "        # x = self.activation(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "\n",
    "        return x\n",
    "        \n",
    "model = CNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "0662dfa8-369d-490a-94c3-5cf560236bf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),  # Resize images to 224x224 (change if needed)\n",
    "        transforms.ToTensor(),          # Convert images to PyTorch tensors\n",
    "        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # Normalize\n",
    "    ])\n",
    "\n",
    "# Define dataset paths\n",
    "train_dir = \"../inaturalist_12k/train\"\n",
    "val_dir = \"../inaturalist_12k/val\"\n",
    "\n",
    "# Load datasets\n",
    "train_dataset = datasets.ImageFolder(root=train_dir, transform=transform)\n",
    "val_dataset = datasets.ImageFolder(root=val_dir, transform=transform)\n",
    "\n",
    "# Check class mapping (optional)\n",
    "# print(\"Class names:\", train_dataset.classes)  # List of class names\n",
    "# print(\"Class indices:\", train_dataset.class_to_idx)  # Mapping class → index\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "# device = torch.device('cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "10a9c935-e05c-4d2c-b5bd-6bac9f51829e",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-4\n",
    "batch_size = 64\n",
    "max_epochs = 5\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size, shuffle=False, num_workers=4, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "5c4dc871-487f-4ff6-a27b-aaaaef3bd99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss() # Loss function\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate) # Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ca6a2d-e4b8-4365-8149-423ac2080b63",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                            | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,    10] loss: 2.301\n",
      "[1,    20] loss: 2.302\n",
      "[1,    30] loss: 2.299\n",
      "[1,    40] loss: 2.294\n",
      "[1,    50] loss: 2.275\n",
      "[1,    60] loss: 2.289\n",
      "[1,    70] loss: 2.256\n",
      "[1,    80] loss: 2.237\n",
      "[1,    90] loss: 2.224\n",
      "[1,   100] loss: 2.197\n",
      "[1,   110] loss: 2.218\n",
      "[1,   120] loss: 2.198\n",
      "[1,   130] loss: 2.179\n",
      "[1,   140] loss: 2.165\n",
      "[1,   150] loss: 2.208\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██████████▍                                         | 1/5 [00:51<03:26, 51.71s/it]"
     ]
    }
   ],
   "source": [
    "model.to(device)\n",
    "for epoch in tqdm(range(max_epochs)):\n",
    "    running_loss = 0\n",
    "    counter = 0\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        pred_labels = model(images)\n",
    "        loss = loss_fn(pred_labels, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        if counter % 10 == 9:\n",
    "            print(f'[{epoch + 1}, {counter + 1:5d}] loss: {running_loss / 10:.3f}')\n",
    "            running_loss = 0.0\n",
    "        counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "503a9d99-1318-44b5-ac6a-0fd56965c9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "# since we're not training, we don't need to calculate the gradients for our outputs\n",
    "with torch.no_grad():\n",
    "    for images, labels in val_loader:\n",
    "        # calculate outputs by running images through the network\n",
    "        pred_labels = model(images)\n",
    "        # the class with the highest energy is what we choose as prediction\n",
    "        _, predicted = torch.max(pred_labels, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Accuracy of the network on the 10000 test images: {100 * correct // total} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec45fd0-a232-4197-8d13-dd0bf12e9b02",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (da6401)",
   "language": "python",
   "name": "da6401"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
