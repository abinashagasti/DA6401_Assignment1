{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "9c4c45d1-ed26-48ca-a866-eca3316fa2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.multiprocessing as mp\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "3c81e930-45f3-47bf-bd15-30b117b0b58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, num_filters=[32, 64, 128, 256, 512], kernel_size=3, num_dense=256, num_classes=10, activation=F.relu):\n",
    "        super(CNN, self).__init__()\n",
    "        self.activation = activation  # Set activation function\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        # Convolutional layers\n",
    "        self.convs = nn.ModuleList()\n",
    "        in_channels = 3  # Initial input channels (RGB)\n",
    "        for out_channels in num_filters:\n",
    "            self.convs.append(nn.Conv2d(in_channels, out_channels, kernel_size))\n",
    "            in_channels = out_channels  # Update input channels for next layer\n",
    "\n",
    "        # Dummy input to calculate flattened size\n",
    "        self.flattened_size = self._get_flattened_size((3, 224, 224))\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(self.flattened_size, num_dense)\n",
    "        self.fc2 = nn.Linear(num_dense, num_classes)\n",
    "\n",
    "    def _get_flattened_size(self, input_shape):\n",
    "        \"\"\"Passes a dummy tensor through conv layers to compute flattened size.\"\"\"\n",
    "        with torch.no_grad():\n",
    "            x = torch.zeros(1, *input_shape)\n",
    "            for conv in self.convs:\n",
    "                x = self.pool(self.activation(conv(x)))\n",
    "            return x.numel()\n",
    "\n",
    "    def forward(self, x):\n",
    "        for conv in self.convs:\n",
    "            x = self.pool(self.activation(conv(x)))\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        x = self.activation(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "        \n",
    "model = CNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "b413b4ab-3999-4206-90c3-eb2579615773",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 256])"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(1,512,5,5)\n",
    "x = torch.flatten(x, 1)\n",
    "x = model.fc1(x)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "0662dfa8-369d-490a-94c3-5cf560236bf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),  # Resize images to 224x224 (change if needed)\n",
    "        transforms.ToTensor(),          # Convert images to PyTorch tensors\n",
    "        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # Normalize\n",
    "    ])\n",
    "\n",
    "# Define dataset paths\n",
    "train_dir = \"../inaturalist_12k/train\"\n",
    "val_dir = \"../inaturalist_12k/val\"\n",
    "\n",
    "# Load datasets\n",
    "train_dataset = datasets.ImageFolder(root=train_dir, transform=transform)\n",
    "val_dataset = datasets.ImageFolder(root=val_dir, transform=transform)\n",
    "\n",
    "# Check class mapping (optional)\n",
    "# print(\"Class names:\", train_dataset.classes)  # List of class names\n",
    "# print(\"Class indices:\", train_dataset.class_to_idx)  # Mapping class → index\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "# device = torch.device('cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "10a9c935-e05c-4d2c-b5bd-6bac9f51829e",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "batch_size = 64\n",
    "max_epochs = 5\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size, shuffle=False, num_workers=4, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "5c4dc871-487f-4ff6-a27b-aaaaef3bd99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss() # Loss function\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate) # Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "c8ca6a2d-e4b8-4365-8149-423ac2080b63",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                            | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,    10] loss: 2.622\n",
      "[1,    20] loss: 2.271\n",
      "[1,    30] loss: 2.192\n",
      "[1,    40] loss: 2.179\n",
      "[1,    50] loss: 2.149\n",
      "[1,    60] loss: 2.133\n",
      "[1,    70] loss: 2.097\n",
      "[1,    80] loss: 2.080\n",
      "[1,    90] loss: 2.126\n",
      "[1,   100] loss: 2.078\n",
      "[1,   110] loss: 2.093\n",
      "[1,   120] loss: 2.058\n",
      "[1,   130] loss: 2.023\n",
      "[1,   140] loss: 2.026\n",
      "[1,   150] loss: 2.030\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██████████▍                                         | 1/5 [00:54<03:38, 54.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2,    10] loss: 2.019\n",
      "[2,    20] loss: 2.025\n",
      "[2,    30] loss: 1.972\n",
      "[2,    40] loss: 2.002\n",
      "[2,    50] loss: 1.977\n",
      "[2,    60] loss: 2.015\n",
      "[2,    70] loss: 1.984\n",
      "[2,    80] loss: 1.964\n",
      "[2,    90] loss: 1.939\n",
      "[2,   100] loss: 1.970\n",
      "[2,   110] loss: 2.008\n",
      "[2,   120] loss: 1.928\n",
      "[2,   130] loss: 1.996\n",
      "[2,   140] loss: 2.000\n",
      "[2,   150] loss: 1.952\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████████████████████▊                               | 2/5 [01:47<02:41, 53.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3,    10] loss: 1.924\n",
      "[3,    20] loss: 1.871\n",
      "[3,    30] loss: 1.917\n",
      "[3,    40] loss: 2.025\n",
      "[3,    50] loss: 1.925\n",
      "[3,    60] loss: 1.876\n",
      "[3,    70] loss: 1.897\n",
      "[3,    80] loss: 1.885\n",
      "[3,    90] loss: 1.894\n",
      "[3,   100] loss: 1.880\n",
      "[3,   110] loss: 1.912\n",
      "[3,   120] loss: 1.884\n",
      "[3,   130] loss: 1.950\n",
      "[3,   140] loss: 1.956\n",
      "[3,   150] loss: 1.894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|███████████████████████████████▏                    | 3/5 [02:40<01:46, 53.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4,    10] loss: 1.803\n",
      "[4,    20] loss: 1.848\n",
      "[4,    30] loss: 1.828\n",
      "[4,    40] loss: 1.898\n",
      "[4,    50] loss: 1.834\n",
      "[4,    60] loss: 1.790\n",
      "[4,    70] loss: 1.895\n",
      "[4,    80] loss: 1.857\n",
      "[4,    90] loss: 1.812\n",
      "[4,   100] loss: 1.794\n",
      "[4,   110] loss: 1.835\n",
      "[4,   120] loss: 1.882\n",
      "[4,   130] loss: 1.818\n",
      "[4,   140] loss: 1.913\n",
      "[4,   150] loss: 1.875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|█████████████████████████████████████████▌          | 4/5 [03:33<00:53, 53.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5,    10] loss: 1.761\n",
      "[5,    20] loss: 1.742\n",
      "[5,    30] loss: 1.756\n",
      "[5,    40] loss: 1.741\n",
      "[5,    50] loss: 1.790\n",
      "[5,    60] loss: 1.788\n",
      "[5,    70] loss: 1.765\n",
      "[5,    80] loss: 1.827\n",
      "[5,    90] loss: 1.778\n",
      "[5,   100] loss: 1.798\n",
      "[5,   110] loss: 1.875\n",
      "[5,   120] loss: 1.739\n",
      "[5,   130] loss: 1.825\n",
      "[5,   140] loss: 1.797\n",
      "[5,   150] loss: 1.753\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████| 5/5 [04:25<00:00, 53.17s/it]\n"
     ]
    }
   ],
   "source": [
    "model.to(device)\n",
    "for epoch in tqdm(range(max_epochs)):\n",
    "    running_loss = 0\n",
    "    counter = 0\n",
    "    model.train()\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        pred_labels = model(images)\n",
    "        loss = loss_fn(pred_labels, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        if counter % 10 == 9:\n",
    "            print(f'[{epoch + 1}, {counter + 1:5d}] loss: {running_loss / 10:.3f}')\n",
    "            running_loss = 0.0\n",
    "        counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "503a9d99-1318-44b5-ac6a-0fd56965c9a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 34 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "# since we're not training, we don't need to calculate the gradients for our outputs\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    for images, labels in val_loader:\n",
    "        # calculate outputs by running images through the network\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        pred_labels = model(images)\n",
    "        # the class with the highest energy is what we choose as prediction\n",
    "        _, predicted = torch.max(pred_labels, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Accuracy of the network on the 10000 test images: {100 * correct // total} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec45fd0-a232-4197-8d13-dd0bf12e9b02",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (da6401)",
   "language": "python",
   "name": "da6401"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
